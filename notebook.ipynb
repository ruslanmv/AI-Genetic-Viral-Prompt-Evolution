{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Genetic Viral Prompt Evolution 🧬\n",
    "\n",
    "Prompt engineering is often described as a dark art. Crafting the perfect set of instructions to reliably get the desired output from a Large Language Model (LLM) can feel like a mix of intuition, guesswork, and endless trial and error. But what if we could apply a scientific, data-driven process to this challenge? What if we could *evolve* the perfect prompt automatically?\n",
    "\n",
    "In this notebook, we'll explore a novel algorithm that does just that. Inspired by the principles of genetic mutation and viral evolution, this system iteratively refines an LLM prompt against a ground-truth dataset, automatically discovering the most effective instructions.\n",
    "\n",
    "We'll walk through the entire project, built to run on **IBM watsonx.ai**. You'll get the full project structure, all the Python code, and a step-by-step guide to run it yourself right here in this Colab environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Core Concept: How It Works\n",
    "\n",
    "At its heart, the algorithm treats a prompt as a “genome” — a set of instructions that can be tested for fitness, mutated, and improved over generations. The process is visualized in the flowchart below, which outlines the logical steps of the evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph TD\n",
    "    A[Start] --> B[Initialize Seed Prompt, Reward Score=0, Failure Log];\n",
    "    B --> C{For each row in Ground Truth Dataset};\n",
    "    C --> D[Generate Outcome with LLM using Current Prompt];\n",
    "    D --> E{Does Outcome Match Ground Truth?};\n",
    "\n",
    "    E -- Yes --> F[Success: Increment Prompt's Reward Score];\n",
    "    F --> G[Reset Consecutive Failures];\n",
    "    G --> H{Move to Next Row};\n",
    "    H --> C;\n",
    "\n",
    "    E -- No --> I[Failure: Increment Consecutive Failure Counter];\n",
    "    I --> J{Is Failure Count < 20?};\n",
    "\n",
    "    J -- Yes: Mutate --> K[Apply Slight Mutation to Prompt];\n",
    "    K --> L{Check: Prompt < 2000 Tokens};\n",
    "    L -- Yes --> D;\n",
    "    L -- No --> K;\n",
    "\n",
    "    J -- No: Adapt --> M[Log & Summarize 20 Failure Reasons];\n",
    "    M --> N[Make Strategic, Informed Prompt Adjustment];\n",
    "    N --> O[Reset Consecutive Failures];\n",
    "    O --> H;\n",
    "\n",
    "    C -- All Rows Processed --> P[End];\n",
    "    P --> Q[Output Final Optimized Prompt];\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolutionary Journey 🧬\n",
    "\n",
    "  * **Initialization** – We start with a `Seed Prompt`, a `Reward Score` of 0, and an empty `Failure Log`.\n",
    "  * **Environment** – The system iterates through a `Ground Truth Dataset`, which provides the challenges to test our prompt's fitness.\n",
    "  * **Test** – For each row, the current prompt is used to `Generate an Outcome` from the LLM.\n",
    "  * **Fitness Check**:\n",
    "      * **Success** → The prompt's `Reward Score` is incremented, and the system moves to the next row, keeping the successful prompt.\n",
    "      * **Failure** → The `Consecutive Failure Counter` for the current row is incremented.\n",
    "  * **Micro-evolution (The \"Mutate\" Loop)** – If the failure count is less than 20, the system applies a `Slight Mutation` to the prompt and retries the *same row*. This allows for minor adjustments to overcome a specific hurdle.\n",
    "  * **Macro-evolution (The \"Adapt\" Path)** – After 20 straight failures on one row, a deeper learning process is triggered. The system will `Log & Summarize` the reasons for the 20 failures. Based on this summary, it makes a `Strategic, Informed Prompt Adjustment` before moving to the next row.\n",
    "  * **Champion** – After processing all rows, the algorithm outputs the `Final Optimized Prompt`—the one that proved most successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Project Setup\n",
    "\n",
    "First, we'll install the necessary Python libraries. Then, we'll use `%%writefile` magic commands to create all the configuration and code files needed for the project directly in our Colab environment. This makes the project self-contained and easy to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "!pip install beeai_framework ibm-watson-machine-learning pandas numpy tiktoken python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Configuration (`config_watsonx.toml`)\n",
    "\n",
    "This TOML file holds all the tunable parameters for your experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "write_config"
   },
   "outputs": [],
   "source": [
    "%%writefile config_watsonx.toml\n",
    "\n",
    "[data]\n",
    "classes = [\"A\",\"B\",\"C\",\"D\"]\n",
    "\n",
    "[genome]\n",
    "header  = \"You are a strict classifier. Answer with a single letter.\"\n",
    "context = \"\"\"\n",
    "EX1 -> A\n",
    "EX2 -> B\n",
    "EX3 -> D\n",
    "\"\"\"\n",
    "query   = \"Predict the class for the row: {{ROW}}\"\n",
    "\n",
    "[evolver]\n",
    "max_prompt_tokens = 2000\n",
    "max_tries   = 20\n",
    "fail_memory = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Environment Variables (`.env`)\n",
    "\n",
    "Create this file to store your credentials securely. \n",
    "**Important:** Replace the placeholder values with your actual IBM Watsonx credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "write_env"
   },
   "outputs": [],
   "source": [
    "%%writefile .env\n",
    "\n",
    "# Watsonx credentials (KEEP PRIVATE)\n",
    "WATSONX_PROJECT_ID=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx\n",
    "WATSONX_API_KEY=************************\n",
    "WATSONX_API_URL=https://us-south.ml.cloud.ibm.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Ground Truth Data (`my_data.csv`)\n",
    "\n",
    "This is our ground truth dataset. The evolver will iterate through this file to test and refine the prompts. We'll create a small sample file for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "write_data"
   },
   "outputs": [],
   "source": [
    "%%writefile my_data.csv\n",
    "payload,truth\n",
    "\"Input text for classification task 1\",A\n",
    "\"Another piece of data for the model\",B\n",
    "\"This one should be classified as C\",C\n",
    "\"A different kind of input string\",D\n",
    "\"Sample text that belongs to class A\",A\n",
    "\"More data for testing the B category\",B\n",
    "\"Final test case for class C\",C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. The Engine (`prompt_evolver_watsonx.py`)\n",
    "\n",
    "This is the complete, fully asynchronous Python script. It implements the logic from the flowchart, using specific techniques like an `immunity` counter to fulfill the \"Log & Summarize\" step and a mutator probability adjustment for the \"Strategic Adjustment\" step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "write_engine"
   },
   "outputs": [],
   "source": [
    "%%writefile prompt_evolver_watsonx.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Genetic-Viral Prompt Evolver  –  Watson X edition\n",
    "Author: ruslanmv.com – 2025-06-23\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import asyncio, os, sys, random, re, hashlib, tomllib, pathlib\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, replace\n",
    "from typing import Dict, List, Tuple, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ── Watsonx.ai wiring ──────────────────────────────────────────────────────\n",
    "try:\n",
    "    from beeai_framework.adapters.watsonx import WatsonxChatModel\n",
    "    from beeai_framework.backend import UserMessage, ChatModel\n",
    "    from beeai_framework.errors import FrameworkError\n",
    "except ImportError:\n",
    "    print(\"❌  beeai_framework not installed.  `pip install beeai_framework`\",\n",
    "          file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "BASE_DIR = pathlib.Path().resolve()\n",
    "load_dotenv(BASE_DIR / \".env\")\n",
    "\n",
    "SEM = asyncio.Semaphore(8)  # Watsonx: 8 queries per second\n",
    "\n",
    "try:\n",
    "    CHAT: ChatModel = WatsonxChatModel(\n",
    "        model_id=\"meta-llama/llama-3-8b-instruct\", # Using a more common Llama 3 model\n",
    "        settings={\n",
    "            \"temperature\": 0.1,\n",
    "            \"top_p\": 0.9,\n",
    "            \"project_id\": os.getenv(\"WATSONX_PROJECT_ID\"),\n",
    "            \"api_key\":    os.getenv(\"WATSONX_API_KEY\"),\n",
    "            \"api_base\":   os.getenv(\"WATSONX_API_URL\"),\n",
    "        },\n",
    "    )\n",
    "except FrameworkError as e:\n",
    "    print(f\"❌  Cannot init WatsonxChatModel: {e.explain()}\", file=sys.stderr)\n",
    "    sys.exit(1)\n",
    "\n",
    "async def watsonx_call(prompt: str) -> str:\n",
    "    \"\"\"Rate-limited chat call.\"\"\"\n",
    "    async with SEM:\n",
    "        try:\n",
    "            resp = await CHAT.create(messages=[UserMessage(prompt)])\n",
    "            return resp.get_text_content().strip()\n",
    "        except FrameworkError as e:\n",
    "            print(\"Watsonx error:\", e.explain(), file=sys.stderr); return \"\"\n",
    "        except Exception as e:\n",
    "            print(\"Unexpected Watsonx error:\", e, file=sys.stderr);  return \"\"\n",
    "\n",
    "# ── Config ────────────────────────────────────────────────────────────────\n",
    "CFG = tomllib.loads((BASE_DIR / \"config_watsonx.toml\").read_text())\n",
    "ENC = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")  # rough length estimate\n",
    "\n",
    "# ── Genome dataclass ──────────────────────────────────────────────────────\n",
    "@dataclass(frozen=True)\n",
    "class Genome:\n",
    "    header: str; context: str; payload: str; query: str\n",
    "\n",
    "    def render(self) -> str:\n",
    "        return \"\\n\\n\".join([self.header, self.context, self.payload, self.query])\n",
    "\n",
    "    def token_len(self) -> int:\n",
    "        return len(ENC.encode(self.render()))\n",
    "\n",
    "    # ---- Evolutionary operators -----------------------------------------\n",
    "    def mutate(self, immunity: Counter) -> \"Genome\":\n",
    "        ops, probs = zip(*[(k, v[\"p\"]) for k, v in MUTATORS.items()])\n",
    "        probs = np.array(probs, dtype=float)\n",
    "        for i, op in enumerate(ops):\n",
    "            probs[i] *= 0.5 ** immunity[op]  # immunity penalty\n",
    "        if probs.sum() == 0: # Avoid division by zero if all probabilities are penalized to zero\n",
    "            probs = np.ones(len(ops)) / len(ops)\n",
    "        else:\n",
    "            probs /= probs.sum()\n",
    "        op = np.random.choice(ops, p=probs)\n",
    "        return MUTATORS[op][\"fn\"](self)\n",
    "\n",
    "    def hash(self) -> str:\n",
    "        return hashlib.sha1(self.render().encode()).hexdigest()[:10]\n",
    "\n",
    "# ── Mutation operators ────────────────────────────────────────────────────\n",
    "def synonym_flip(g: Genome) -> Genome:\n",
    "    verbs = [\"Predict\", \"Provide\", \"Return\", \"Classify\", \"Identify\"]\n",
    "    return replace(g, query=re.sub(r\"^\\w+\", random.choice(verbs), g.query))\n",
    "\n",
    "def example_dropout(g: Genome) -> Genome:\n",
    "    blocks = g.context.split(\"\\n\\n\")\n",
    "    if len(blocks) > 1:\n",
    "        blocks.pop(random.randrange(len(blocks)))\n",
    "    return replace(g, context=\"\\n\\n\".join(blocks))\n",
    "\n",
    "def temp_tweak(g: Genome) -> Genome:\n",
    "    if \"temperature=\" in g.query:\n",
    "        q = re.sub(r\"temperature=\\d(\\.\\d+)?\", \"temperature=0.0\", g.query)\n",
    "    else:\n",
    "        q = f\"{g.query} temperature=0.0\"\n",
    "    return replace(g, query=q)\n",
    "\n",
    "MUTATORS: Dict[str, Dict[str, Any]] = {\n",
    "    \"synonym_flip\":    {\"p\": .30, \"fn\": synonym_flip},\n",
    "    \"example_dropout\": {\"p\": .40, \"fn\": example_dropout},\n",
    "    \"temp_tweak\":      {\"p\": .30, \"fn\": temp_tweak},\n",
    "}\n",
    "\n",
    "# ── LLM helpers ───────────────────────────────────────────────────────────\n",
    "async def classify(prompt: str, row_payload: str) -> str:\n",
    "    return await watsonx_call(prompt.replace(\"{{ROW}}\", row_payload))\n",
    "\n",
    "def post_process(raw: str) -> str:\n",
    "    raw = raw.strip().upper()\n",
    "    for c in CFG[\"data\"][\"classes\"]:\n",
    "        if c in raw: return c # Simple check\n",
    "    return \"UNKNOWN\"\n",
    "\n",
    "async def diagnose_failure(raw: str, truth: str) -> str:\n",
    "    diag = await watsonx_call(\n",
    "        f\"You produced: {raw}\\nCorrect: {truth}\\n\"\n",
    "        \"Give the main reason for the error in 5 words or less.\")\n",
    "    return diag or \"unknown_cause\"\n",
    "\n",
    "def reward(attempt: int) -> float:  # 1/attempt\n",
    "    return 1.0 / attempt\n",
    "\n",
    "# ── Evolution loop ────────────────────────────────────────────────────────\n",
    "async def evolve(seed: Genome, df: pd.DataFrame) -> Tuple[Genome, List[float]]:\n",
    "    champ, genome = seed, seed\n",
    "    rewards: List[float] = []\n",
    "    immunity = Counter(); consec_fail = 0\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        print(f\"\\n--- Row {idx+1}/{len(df)} ---\")\n",
    "        fail_reasons_for_row = []\n",
    "        for attempt in range(1, CFG[\"evolver\"][\"max_tries\"] + 1):\n",
    "            print(f\"Attempt {attempt}: Genome hash {genome.hash()}\")\n",
    "            raw_pred = await classify(genome.render(), row[\"payload\"])\n",
    "            pred = post_process(raw_pred)\n",
    "            \n",
    "            if pred == row[\"truth\"]:\n",
    "                current_reward = reward(attempt)\n",
    "                rewards.append(current_reward)\n",
    "                print(f\"✅ Success! Prediction '{pred}' matched truth. Reward: {current_reward:.2f}\")\n",
    "                consec_fail = 0\n",
    "                champ = genome\n",
    "                break\n",
    "            \n",
    "            # ---- failure branch: implements the \"No\" path from the flowchart\n",
    "            consec_fail += 1\n",
    "            reason = await diagnose_failure(raw_pred, row[\"truth\"])\n",
    "            fail_reasons_for_row.append(reason)\n",
    "            print(f\"❌ Failure. Pred: '{pred}', Truth: '{row['truth']}'. Reason: {reason}\")\n",
    "            \n",
    "            # This 'immunity' counter is our implementation of the \"Log\" step\n",
    "            immunity[reason] += 1\n",
    "\n",
    "            # This is the \"Slight Mutation\" step\n",
    "            original_genome = genome\n",
    "            while True:\n",
    "                genome = original_genome.mutate(immunity)\n",
    "                if genome.token_len() <= CFG[\"evolver\"][\"max_prompt_tokens\"]:\n",
    "                    break\n",
    "                print(\"  Mutation resulted in oversized prompt, retrying mutation...\")\n",
    "\n",
    "            # This implements the \"Adapt\" path after N failures\n",
    "            if consec_fail >= CFG[\"evolver\"][\"fail_memory\"]:\n",
    "                print(f\"🔥 Hit failure memory limit ({consec_fail} fails). Adapting strategy.\")\n",
    "                adjust_mutator(Counter(fail_reasons_for_row))\n",
    "                immunity.clear()\n",
    "                consec_fail = 0\n",
    "                break\n",
    "        else: # This else belongs to the for loop, executes if loop finishes without break\n",
    "            print(\"🚫 Exhausted all attempts for this row.\")\n",
    "            rewards.append(0.0)  # exhausted attempts\n",
    "    return champ, rewards\n",
    "\n",
    "# ── Mutator adjustment ────────────────────────────────────────────────────\n",
    "def adjust_mutator(fails: Counter):\n",
    "    \"\"\"This function is our implementation of the \"Strategic Adjustment\" step.\"\"\"\n",
    "    if not fails: return\n",
    "    print(\"\\n🧬 Adjusting mutator probabilities based on recent failures:\")\n",
    "    print(f\"   Failure reasons: {fails}\")\n",
    "    tot = sum(fails.values())\n",
    "    for op, cnt in fails.items():\n",
    "        # Heuristic: if a failure reason is highly correlated, adjust a related operator\n",
    "        # This part is simple; a more complex mapping could be built.\n",
    "        related_op = \"example_dropout\" if \"example\" in op.lower() else \"synonym_flip\"\n",
    "        if related_op in MUTATORS and cnt / tot >= 0.6:\n",
    "            print(f\"   High correlation for '{op}', penalizing '{related_op}' mutator.\")\n",
    "            MUTATORS[related_op][\"p\"] *= 0.5\n",
    "            \n",
    "    s = sum(v[\"p\"] for v in MUTATORS.values())\n",
    "    for v in MUTATORS.values(): v[\"p\"] /= s\n",
    "    print(f\"   New probabilities: { {k: f'{v['p']:.2f}' for k, v in MUTATORS.items()} }\")\n",
    "\n",
    "# ── CLI ───────────────────────────────────────────────────────────────────\n",
    "async def _async_main(dataset: str):\n",
    "    df = pd.read_csv(dataset)\n",
    "    seed = Genome(CFG[\"genome\"][\"header\"], CFG[\"genome\"][\"context\"],\n",
    "                  \"{{ROW}}\", CFG[\"genome\"][\"query\"])\n",
    "    \n",
    "    print(\"🌱 Initial Seed Prompt:\")\n",
    "    print(seed.render())\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    champ, rewards = await evolve(seed, df)\n",
    "    out = BASE_DIR / \"evolution_out_watsonx\"; out.mkdir(exist_ok=True)\n",
    "    (out / \"champion_prompt.txt\").write_text(champ.render())\n",
    "    pd.DataFrame({\"reward\": rewards}).to_csv(out / \"reward_log.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"\\n🏆 Evolution Complete! Champion hash: {champ.hash()}, Total reward: {sum(rewards):.2f}\")\n",
    "    print(\"\\nChampion Prompt saved to evolution_out_watsonx/champion_prompt.txt:\")\n",
    "    print(\"-\"*20)\n",
    "    print(champ.render())\n",
    "    print(\"-\"*20)\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "    # In Colab, we don't use command-line args, so we'll hardcode the dataset path\n",
    "    dataset_path = 'my_data.csv'\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"❌ Dataset not found: {dataset_path}\", file=sys.stderr); sys.exit(1)\n",
    "    # Use a try-except block to handle running in an environment with an active event loop (like Colab)\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:  # 'RuntimeError: There is no current event loop...'\n",
    "        loop = None\n",
    "\n",
    "    if loop and loop.is_running():\n",
    "        print('Async event loop already running. Adding main coroutine to the loop.')\n",
    "        loop.create_task(_async_main(dataset_path))\n",
    "    else:\n",
    "        print('Starting new async event loop.')\n",
    "        asyncio.run(_async_main(dataset_path))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the Evolver\n",
    "\n",
    "Now that all the files are created, we can execute the main script. \n",
    "\n",
    "**Before you run:** Make sure you have edited the `.env` file cell (Section 1.2) with your actual Watsonx credentials. The script will fail if the credentials are not valid.\n",
    "\n",
    "The script will print its progress as it iterates through the dataset, showing successes, failures, and mutations. This will take some time depending on the size of the dataset and the latency of the Watsonx API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_evolver"
   },
   "outputs": [],
   "source": [
    "!python prompt_evolver_watsonx.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: View the Results\n",
    "\n",
    "After the script finishes, the outputs will land in the `evolution_out_watsonx/` directory.\n",
    "\n",
    "| File | What it is |\n",
    "| :--- | :--- |\n",
    "| **`champion_prompt.txt`** | The fittest prompt genome. |\n",
    "| **`reward_log.csv`** | Per-row reward curve — ready for plotting. |\n",
    "\n",
    "We can use shell commands to view the contents of these files directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_champion"
   },
   "outputs": [],
   "source": [
    "print(\"--- Champion Prompt ---\")\n",
    "!cat evolution_out_watsonx/champion_prompt.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "view_rewards"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"\\n--- Reward Log ---\")\n",
    "reward_df = pd.read_csv('evolution_out_watsonx/reward_log.csv')\n",
    "print(reward_df)\n",
    "\n",
    "# Optional: Plot the rewards\n",
    "reward_df['cumulative_reward'] = reward_df['reward'].cumsum()\n",
    "reward_df['cumulative_reward'].plot(title='Cumulative Reward Over Evolution', grid=True, figsize=(10, 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond the Basics 🧪\n",
    "\n",
    "This framework is built to be extended. Here are a few ideas:\n",
    "\n",
    "  * **Smarter rewards** – use cosine-similarity between embeddings for partial credit.\n",
    "  * **Parallel lineages** – spawn multiple processes sharing a Redis “immunity” buffer.\n",
    "  * **Richer mutations** – inject schema hints, reorder headers, or let the LLM rewrite itself.\n",
    "  * **Dashboard** – plot `reward_log.csv` in Grafana to watch evolution live.\n",
    "\n",
    "This evolutionary approach transforms prompt engineering from a manual chore into an automated, data-driven optimisation problem. Happy tinkering! 🐣"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
